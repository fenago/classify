{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import your Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 1\n",
    "# Load your data  -- start with CreditScoring.csv... then online retail\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/fenago/classify/main/data/CreditScoring.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "This session covers data collection and some procedures of data preparation. \n",
    "\n",
    "**Commands, functions, and methods:** \n",
    "\n",
    "* `!wget` - Linux shell command for downloading data \n",
    "* `pd.read.csv()` - read csv files \n",
    "* `df.head()` - take a look of the dataframe \n",
    "* `df.head().T` - take a look of the transposed dataframe \n",
    "* `df.columns` - retrieve column names of a dataframe \n",
    "* `df.columns.str.lower()` - lowercase all the letters \n",
    "* `df.columns.str.replace(' ', '_')` - replace the space separator \n",
    "* `df.dtypes` - retrieve data types of all series \n",
    "* `df.index` - retrive indices of a dataframe\n",
    "* `pd.to_numeric()` - convert a series values to numerical values. The `errors=coerce` argument allows making the transformation despite some encountered errors. \n",
    "* `df.fillna()` - replace NAs with some value \n",
    "* `(df.x == \"yes\").astype(int)` - convert x series of yes-no values to numerical values.\n",
    "* `df['Weight'] = df['Weight'].astype(int)` - this takes a single column of data and converts the data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Data Cleaning\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_') # A\n",
    " \n",
    "string_columns = list(df.dtypes[df.dtypes == 'object'].index) # B\n",
    " \n",
    "for col in string_columns:\n",
    "    df[col] = df[col].str.lower().str.replace(' ', '_') # C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE THAT YOU WRANGLE YOUR DATA.  THIS IS AN EXAMPLE OF THE TYPES OF THINGS THAT ARE NEEDED\n",
    "# SKIP THIS CEL - IT IS ONLY TO REITERATE THE NEED TO CLEAN \n",
    "# For instance - in the CreditScoring dataset - there are numerous 99999999 that need to be replaced\n",
    "# Obviously don't run this with your dataset\n",
    "for c in ['income', 'assets', 'debt']:\n",
    "    df[c] = df[c].replace(to_replace=99999999, value=np.nan)\n",
    "df = df[df.status != 'unk']   # Also make sure to treat the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Values will be encoded with the Dictionary Vectorizor\n",
    "# Numerical Values: At a minimum - clean the missing values and consider scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Visuals so you can gain a business understanding of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your target variable --- df.YOUR_TARGET_VARIABLE  \n",
    "# Look for major data imbalances\n",
    "# Also replace your X label\n",
    "# REPLACE YOUR TARGET VARIABLE\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "sns.histplot(df.status, bins=40, color='black', alpha=1)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('status')\n",
    "plt.title('status')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "* (1) Check for NaN under a single DataFrame column:\n",
    "\n",
    "* `df['your column name'].isnull().values.any()`\n",
    "\n",
    "* (2) Count the NaN under a single DataFrame column:\n",
    "\n",
    "`df['your column name'].isnull().sum()`\n",
    "\n",
    "* (3) Check for NaN under an entire DataFrame:\n",
    "\n",
    "`df.isnull().values.any()`\n",
    "\n",
    "* (4) Count the NaN under an entire DataFrame:\n",
    "\n",
    "`df.isnull().sum().sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls --- you do NOT want nulls when you train\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.status != 0]\n",
    "df.status.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete columns --- this may or may NOT be needed.  As before - skip if you don't need it\n",
    "# You will encounter times where you will want to delete columns.  This is how you do that.\n",
    "# df = df.drop(['x5_latitude', 'x6_longitude', 'x1_transaction_date'], axis=1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test, train, validation sets... 80/20\n",
    "from sklearn.model_selection import train_test_split\n",
    "# This gives the 80/20 train test split\n",
    "df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=11)\n",
    "\n",
    "len(df_train_full), len(df_test)\n",
    "# Replace nulls with 0's - these are pandas dataframes\n",
    "df_train_full = df_train_full.fillna(0)\n",
    "\n",
    "df_test = df_test.fillna(0)\n",
    "len(df_train_full),len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the y out into train/test/splits... these are numpy ndarrays ... msrp is your target variables\n",
    "# Replace with your target variable!!!  \n",
    "y_train = (df_train_full.status).values\n",
    "y_test = (df_test.status).values\n",
    "del df_train_full['status']\n",
    "del df_test['status']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train),len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "* `np.all(np.isfinite(X_train))` - are there any infinite values in the dictionary?\n",
    "* `np.any(np.isnan(X_train))` - are there any NaN in the dictionary?\n",
    "* # Using isna() to select all rows with NaN under an entire array:\n",
    "`df[df.isna().any(axis=1)]`\n",
    "* # Using isnull() to select all rows with NaN under an entire array:\n",
    "`df[df.isnull().any(axis=1)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert these data frames into a LIST of DICTIONARIES (each element in the list is a dictionary (the record))\n",
    "dict_train = df_train_full.to_dict(orient='records')\n",
    "dict_test = df_test.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the LIST OF DICTIONARIES into a Feature Matrix (does all of the encoding)\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    " \n",
    "dv = DictVectorizer(sparse=False)\n",
    " \n",
    "X_train = dv.fit_transform(dict_train)\n",
    "X_test = dv.transform(dict_test)\n",
    "features = dv.get_feature_names()  #Features as they exist in the Vectorized Dictionary (this is an ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms with the Algorithm Harness\n",
    "# https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from time import time\n",
    "from sklearn.metrics import explained_variance_score,mean_absolute_error,r2_score\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'roc_auc'\n",
    "# you can also use other metrics to score your models: https://iqcode.com/code/python/sklearn-cross-val-score-scoring-options\n",
    "for name, model in models:\n",
    "    start = time()\n",
    "    kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time() - start\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    predict_time = time()-start \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    #y_pred = model.predict_proba(X_train)[:, 1]\n",
    "    #auc = roc_auc_score(y_train, y_pred)\n",
    "    print(msg)\n",
    "    print(\"Score for each of the 10 K-fold tests: \",cv_results)\n",
    "    print(model)\n",
    "    print(\"\\tTraining time: %0.3fs\" % train_time)\n",
    "    print(\"\\tPrediction time: %0.3fs\" % predict_time)\n",
    "    #y_pred = model.predict(X_test)\n",
    "    #print(\"\\tExplained variance:\", explained_variance_score(y_test, y_pred))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once you identify a single model or two - begin to investigate\n",
    "## MODIFY THIS TO THE ALGORITHM THAT YOU CHOOSE.  THIS IS AN EXAMPLE WITH A TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 1\n",
    "# if you uncomment %%timeit it will not put lr into memory\n",
    "# Let's assume that the decision tree is the one we want to explore\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give you the list of Hyperparameters of your model\n",
    "dt.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have converted the dataframe into a list of dictionaries - validate the headers\n",
    "type(X_train)\n",
    "type(dv.get_feature_names())\n",
    "type(dt.feature_importances_)\n",
    "dv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the model properties.  You can call all of these\n",
    "def get_properties(model):   \n",
    "  return [i for i in model.__dict__ if i.endswith('_')] \n",
    "get_properties(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough view of the tree - hard to read but can be usable for research.  Will show a clean tree later\n",
    "# from sklearn.tree import export_text \n",
    " \n",
    "# tree_text = export_text(dt, feature_names=dv.feature_names_) \n",
    "# print(tree_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very important.  Once you have a trained model - interegate the coefficients to see WHAT is important\n",
    "feature_names=dv.feature_names_\n",
    "# Evaluate the coefficients to learn what the model thinks is important in the predictions.\n",
    "for i,j in zip(feature_names, dt.feature_importances_): print('%.3f' % j, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix represent predictions vs Actuals on Test Data\n",
    "The predicted data results in the below diagram could be read in the following manner given 1 represents  (positive) - obviously - this will be different if the target has more than 2 options. <br />\n",
    "![image info](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse1.mm.bing.net%2Fth%3Fid%3DOIP.g4tLqo_z92Hk7NZ-JXdziwHaD1%26pid%3DApi&f=1)\n",
    "\n",
    "### True Positive (TP): True positive represents the value of correct predictions of positives out of actual positive cases. \n",
    "### False Positive (FP): False positive represents the value of incorrect positive predictions. \n",
    "### True Negative (TN): True negative represents the value of correct predictions of negatives out of actual negative cases. \n",
    "## False Negative (FN): False negative represents the value of incorrect negative predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "# predict_proba predicts the probability and predict just predicts the category\n",
    "# y_pred = dt.predict_proba(X_test)[:, 1]\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "#\n",
    "# Print the confusion matrix using Matplotlib\n",
    "#\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision: \n",
    "Model precision score represents the model’s ability to correctly predict the positives out of all the positive predictions it made. The precision score is a useful measure of the success of prediction when the classes are very imbalanced. Mathematically, it represents the ratio of true positive to the sum of true positive and false positive.\n",
    "#### Precision Score = TP / (FP + TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision score can be used in the scenario where the machine learning model is required to identify all positive examples without any false positives. For example, machine learning models are used in medical diagnosis applications where the doctor wants machine learning model will not provide a label of pneumonia if the patient does not have this disease. Oncologists want models that can identify all cancerous lesions without any false-positive results, and hence one would use a precision score in such cases.\n",
    "The other example where the precision score can be useful is credit card fraud detection. In credit card fraud detection problems, classification models are evaluated using the precision score to determine how many positive samples were correctly classified by the classification model. You would not like to have a high number of false positives or else you might end up blocking many credit cards and hence a lot of frustrations with the end-users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read this as a percentage - What proportion of positive identifications was actually correct?\n",
    "print('Precision: %.3f' % precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall: \n",
    "Model recall score represents the model’s ability to correctly predict the positives out of actual positives. This is unlike precision which measures how many predictions made by models are actually positive out of all positive predictions made. For example: If your machine learning model is trying to identify positive reviews, the recall score would be what percent of those positive reviews did your machine learning model correctly predict as a positive. In other words, it measures how good our machine learning model is at identifying all actual positives out of all positives that exist within a dataset. The higher the recall score, the better the machine learning model is at identifying both positive and negative examples. Recall score is a useful measure of success of prediction when the classes are very imbalanced.  Mathematically, it represents the ratio of true positive to the sum of true positive and false negative.\n",
    "#### Recall Score = TP / (FN + TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall score can be used in the scenario where the labels are not equally divided among classes. For example, if there is a class imbalance ratio of 20:80 (imbalanced data), then the recall score will be more useful than accuracy because it can provide information about how well the machine learning model identified rarer events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Recall: %.3f' % recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model accuracy:\n",
    "is a machine learning model performance metric that is defined as the ratio of true positives and true negatives to all positive and negative observations. In other words, accuracy tells us how often we can expect our machine learning model will correctly predict an outcome out of the total number of times it made predictions. For example: Let’s assume that you were testing your machine learning model with a dataset of 100 records and that your machine learning model predicted all 90 of those instances correctly. The accuracy metric, in this case, would be: (90/100) = 90%. The accuracy rate is great but it doesn’t tell us anything about the errors our machine learning models make on new data we haven’t seen before.\n",
    "Mathematically, it represents the ratio of the sum of true positive and true negatives out of all the predictions.  Use this measure with extreme caution.  It can be very misleading.\n",
    "#### Accuracy Score = (TP + TN)/ (TP + FN + TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model F1 score:\n",
    "represents the model score as a function of precision and recall score. F-score is a machine learning model performance metric that gives equal weight to both the Precision and Recall for measuring its performance in terms of accuracy, making it an alternative to Accuracy metrics (it doesn’t require us to know the total number of observations). It’s often used as a single value that provides high-level information about the model’s output quality. This is a useful measure of the model in the scenarios where one tries to optimize either of precision or recall score and as a result, the model performance suffers. The following represents the aspects relating to issues with optimizing either precision or recall score:\n",
    "Optimizing for recall helps with minimizing the chance of not detecting a malignant cancer. However, this comes at the cost of predicting malignant cancer in patients although the patients are healthy (a high number of FP).\n",
    "Optimize for precision helps with correctness if the patient has a malignant cancer. However, this comes at the cost of missing malignant cancer more frequently (a high number of FN).\n",
    "Mathematically, it can be represented as harmonic mean of precision and recall score.  Ranges from 0 to 1 and can be read as a percentage when multiplied by 100\n",
    "#### F1 Score = 2* Precision Score * Recall Score/ (Precision Score + Recall Score/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just for the TRUE --- Not the False\n",
    "print('F1 Score: %.3f' % f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look to see if your target variables are balanced.  If they are NOT then consider looking at Recall and Precision\n",
    "# If your target variables are balanced - then consider using accuracy as the defining metric to judge the \"goodness\"\n",
    "df['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same info can be found in the score report\n",
    "# Be ver careful about the data types. predict_proba returns a float\n",
    "# predict returns a different data type\n",
    "from sklearn.metrics import f1_score\n",
    "y_pred = dt.predict_proba(X_test)[:, 1]\n",
    "y_pred = y_pred.astype('float')\n",
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalation Metrics (really more advanced but added here for context)\n",
    "# https://github.com/sepandhaghighi/pycm\n",
    "# Make sure the target variables in the y_test and y_pred are exactly the same units (not 0/1 in one and True/False in the other)\n",
    "!pip install pycm\n",
    "from pycm import ConfusionMatrix\n",
    "cm = ConfusionMatrix(actual_vector=y_test,predict_vector=y_pred)\n",
    "# cm = ConfusionMatrix(y_actu, y_pred, classes=[1,0,4])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification metrics - you can skip or run and print the metric you woult like to look at.\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(cnf_matrix)\n",
    "#[[1 1 3]\n",
    "# [3 2 2]\n",
    "# [1 3 1]]\n",
    "\n",
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "# print(FP)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/((TP+FN)+.01)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/((TN+FP)+.01)\n",
    "# Precision or positive predictive value\n",
    "PPV = TP/((TP+FP)+.01)\n",
    "# Negative predictive value\n",
    "NPV = TN/((TN+FN)+.01)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/((FP+TN)+.01)\n",
    "# False negative rate\n",
    "FNR = FN/((TP+FN)+.01)\n",
    "# False discovery rate\n",
    "FDR = FP/((TP+FP)+.01)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a peek at your predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the first 10 rows and compare the predictions.\n",
    "pred_y = dt.predict(X_test)\n",
    "print(\"The first 10 prediction {}\".format(pred_y[:10].round(0)))\n",
    "print(\"The real first 10 labels {}\".format(y_test[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make a PREDICTION with brand new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_train_full.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use double brackets around the iloc to force it to return a pandas dataframe and not a series\n",
    "# Then you can convert ANY record into a dictionary.\n",
    "# This is important because when you want to make a prediction - it must be in this format\n",
    "df_train_full.iloc[[21]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to convert any pandas row into a dictionary... needed for predictions\n",
    "df_train_full.iloc[[213]].to_dict('records')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to convert any pandas row into a dictionary... needed for predictions\n",
    "df_train_full.head(21).to_dict('records')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item = df_train.head(1).to_dict('records')[0]\n",
    "item = df_train_full.iloc[[213]].to_dict('records')[0]\n",
    "actual = y_train[[213]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The item to be predicted is passed in.  \n",
    "def model_prediction(item, dv, model):\n",
    "    X = dv.transform([item])\n",
    "    y_pred = model.predict(X)\n",
    "    return y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The item to be predicted is passed in.  \n",
    "def model_prediction_proba(item, dv, model):\n",
    "    X = dv.transform([item])\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "    return y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction(item,dv,dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to convert any pandas row into a dictionary... needed for predictions\n",
    "# This will give you a template that you can manipulate to create predictions with new values\n",
    "df_train_full.iloc[[2]].to_dict('records')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list above can be copied into here.  Take everything after a \" : \" and put it in a [] so you can pass it in as a pd.DF\n",
    "# create a DataFrame by transforming scalar values to list   (toggle the weekend)\n",
    "\n",
    "myItem = {'administrative': [6],\n",
    " 'administrative_duration': [94.6],\n",
    " 'informational': [0],\n",
    " 'informational_duration': [2.0],\n",
    " 'productrelated': [15],\n",
    " 'productrelated_duration': [1933.559259],\n",
    " 'bouncerates': [0.005333333],\n",
    " 'exitrates': [0.026377261],\n",
    " 'pagevalues': [167.806338478],\n",
    " 'specialday': [0.0],\n",
    " 'month': ['nov'],\n",
    " 'operatingsystems': [2],\n",
    " 'browser': [2],\n",
    " 'region': [4],\n",
    " 'traffictype': [2],\n",
    " 'visitortype': ['returning_visitor'],\n",
    " 'weekend': [False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF = pd.DataFrame.from_dict(myItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = newDF.to_dict('records')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction(item,dv,dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the model parameters.   You can tune these.\n",
    "dt.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'min_samples_leaf': [20, 3],'max_depth': [4,6,8],'min_samples_split': [2,4]}\n",
    "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3)\n",
    "grid_search_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cv.best_estimator_    # this will output the best values for the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume that the decision tree is the one we want to explore\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=20, random_state=42)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_pred = dt.predict_proba(X_test)[:, 1]\n",
    "y_pred = y_pred.astype('int')\n",
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz( \n",
    " grid_search_cv.best_estimator_,\n",
    " out_file=('tree.dot'),\n",
    " feature_names=dv.get_feature_names(),\n",
    " class_names=None,\n",
    " filled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot\n",
    "import pydot\n",
    "\n",
    "(graph,) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the params by editing the output of this and repeating the above steps.\n",
    "dt.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Many parameters will take a very long time to load\n",
    "param = { 'max_depth': [2,3,5,20,40], \n",
    "         'max_leaf_nodes': [2,20,200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define search\n",
    "search = GridSearchCV(dt, param, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "# execute search\n",
    "result = search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
