{"metadata":{"orig_nbformat":4,"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"#Import your Libraries\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport sklearn.metrics as metrics\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"2dac3d66-b2d4-49df-8540-82d05eeff307"},{"cell_type":"code","source":"# %%timeit -n 1\n# Load your data  -- start with CreditScoring.csv... then online retail\ndf = pd.read_csv('./online_shoppers_intention.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"c21f8a1a-a9d5-4277-868f-bef4e51785a9"},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"823b6b3a-c279-4b3e-9b75-75ab01f8ed0b"},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"dfdc07fd-82d6-411c-bf4e-4012315538e1"},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"4f5bc2f4-a159-4a24-b02d-cf234b7daefa"},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b8057edf-cf08-4bfe-aa36-f56b649f6500"},{"cell_type":"code","source":"df.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"960afc07-cb45-4851-aa9b-6a564d943273"},{"cell_type":"code","source":"df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"17fa6add-bdac-4d33-8b96-e102f3d13bc5"},{"cell_type":"code","source":"# Basic Data Cleaning\ndf.columns = df.columns.str.lower().str.replace(' ', '_') # A\n \nstring_columns = list(df.dtypes[df.dtypes == 'object'].index) # B\n \nfor col in string_columns:\n    df[col] = df[col].str.lower().str.replace(' ', '_') # C","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"03c95958-f524-486f-84dd-62eccffe2985"},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"5694febc-4570-4e82-a40a-4e347e468eb6"},{"cell_type":"code","source":"df.head().T","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"cfd18cea-dbe5-4ac0-864c-922704a8f46a"},{"cell_type":"code","source":"# Categorical Values will be encoded with the Dictionary Vectorizor\n# Numerical Values: At a minimum - clean the missing values and ","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"d7b21bde-5ba4-4806-bbb1-4c750abd7e16"},{"cell_type":"code","source":"# SKIP THIS CEL - IT IS ONLY TO REITERATE THE NEED TO CLEAN \n# For instance - in the CreditScoring dataset - there are numerous 99999999 that need to be replaced\n# Obviously don't run this with your dataset\nfor c in ['income', 'assets', 'debt']:\n    df[c] = df[c].replace(to_replace=99999999, value=np.nan)\ndf = df[df.status != 'unk']   # Also make sure to treat the target variable","metadata":{},"execution_count":null,"outputs":[],"id":"2cbc6fd3-13ac-4947-b727-b15f7cd74520"},{"cell_type":"code","source":"# Replace with your target variable --- df.YOUR_TARGET_VARIABLE  \n# Look for major data imbalances\n# Also replace your X label\nplt.figure(figsize=(6, 4))\n\nsns.histplot(df.revenue, bins=40, color='black', alpha=1)\nplt.ylabel('Frequency')\nplt.xlabel('revenue')\nplt.title('Is revenue')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"227d8604-3160-4070-a1b8-b7c2e71ce771"},{"cell_type":"code","source":"# Check for nulls --- you do NOT want nulls when you train\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"4ad70d6b-e458-4bb7-be22-f550a07aed9f"},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"a8a3347d-5141-4b92-bb07-3dc2dbc1bf3a"},{"cell_type":"code","source":"#delete columns --- this may or may NOT be needed.  As before - skip if you don't need it\n# You will encounter times where you will want to delete columns.  This is how you do that.\n# df = df.drop(['x5_latitude', 'x6_longitude', 'x1_transaction_date'], axis=1)\n# df","metadata":{},"execution_count":null,"outputs":[],"id":"2871e2ab-8677-43b4-bba1-36fe991438ab"},{"cell_type":"code","source":"'''\n# Split the data into test, train, validation sets... 60/20/20\nfrom sklearn.model_selection import train_test_split\n# This gives the 80/20 train test split\ndf_train_full, df_test = train_test_split(df, test_size=0.2, random_state=11)\n# This splits df_train_full again so it is 60/20/20\ndf_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=11)\nlen(df_train), len(df_val), len(df_test)\n# Replace nulls with 0's - these are pandas dataframes\ndf_train = df_train.fillna(0)\ndf_val = df_val.fillna(0)\ndf_test = df_test.fillna(0)\nlen(df_train),len(df_val),len(df_test)\n'''","metadata":{},"execution_count":null,"outputs":[],"id":"fe1b5783-3f32-4440-81b7-1ae401f4ef09"},{"cell_type":"code","source":"# Split the data into test, train, validation sets... 80/20\nfrom sklearn.model_selection import train_test_split\n# This gives the 80/20 train test split\ndf_train_full, df_test = train_test_split(df, test_size=0.2, random_state=11)\n\nlen(df_train_full), len(df_test)\n# Replace nulls with 0's - these are pandas dataframes\ndf_train_full = df_train_full.fillna(0)\n\ndf_test = df_test.fillna(0)\nlen(df_train_full),len(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"79babb7b-c583-4831-b555-54e68361b295"},{"cell_type":"code","source":"#Split the y out into train/test/splits... these are numpy ndarrays ... msrp is your target variables\n# Replace with your target variable!!!  \ny_train = (df_train_full.revenue).values\ny_test = (df_test.revenue).values\ndel df_train_full['revenue']\ndel df_test['revenue']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"033173c4-f018-4f63-b21b-f04209d392f7"},{"cell_type":"code","source":"len(y_train),len(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"cb9de144-603a-4450-892a-6d724878ed1a"},{"cell_type":"code","source":"# Convert these data frames into a LIST of DICTIONARIES (each element in the list is a dictionary (the record))\ndict_train = df_train_full.to_dict(orient='records')\ndict_test = df_test.to_dict(orient='records')","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"65eccd7c-38b2-4c32-9408-1c3e4eeb78e9"},{"cell_type":"code","source":"# Convert the LIST OF DICTIONARIES into a Feature Matrix (does all of the encoding)\nfrom sklearn.feature_extraction import DictVectorizer\n \ndv = DictVectorizer(sparse=False)\n \nX_train = dv.fit_transform(dict_train)\nX_test = dv.transform(dict_test)\nfeatures = dv.get_feature_names_out()  #Features as they exist in the Vectorized Dictionary (this is an ndarray)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8430e24e-0e56-4b06-8b2b-465762403e19"},{"cell_type":"code","source":"X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"e78f7c8e-e9f3-4a18-8d3c-b64873597b52"},{"cell_type":"code","source":"# Compare Algorithms\nfrom sklearn.metrics import roc_auc_score\nfrom time import time\nfrom sklearn.metrics import explained_variance_score,mean_absolute_error,r2_score\nfrom pandas import read_csv\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    start = time()\n    kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n    model.fit(X_train, y_train)\n    train_time = time() - start\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    predict_time = time()-start \n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    #y_pred = model.predict_proba(X_train)[:, 1]\n    #auc = roc_auc_score(y_train, y_pred)\n    print(msg)\n    print(\"Score for each of the 10 K-fold tests: \",cv_results)\n    print(model)\n    print(\"\\tTraining time: %0.3fs\" % train_time)\n    print(\"\\tPrediction time: %0.3fs\" % predict_time)\n    #y_pred = model.predict(X_test)\n    #print(\"\\tExplained variance:\", explained_variance_score(y_test, y_pred))\n    print()\n    \n    \n    \n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"3c6ec7d7-b95f-496a-9b4b-82ddabb0b1ab"},{"cell_type":"markdown","source":"# Once you identify a single model or two - begin to investigate","metadata":{},"id":"a1e2fe12-ea3d-4e2d-965b-8e4d1c0dd661"},{"cell_type":"code","source":"# %%timeit -n 1\n# if you uncomment %%timeit it will not put lr into memory\n# Let's assume that the decision tree is the one we want to explore\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"6d025def-452c-4def-807f-1da7523f0b60"},{"cell_type":"code","source":"# This will give you the list of Hyperparameters of your model\ndt.get_params()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"855abbd3-905c-4c96-93d1-c08ab4c495a9"},{"cell_type":"code","source":"# You have converted the dataframe into a list of dictionaries - validate the headers\ntype(X_train)\ntype(dv.get_feature_names_out())\ntype(dt.feature_importances_)\ndv.get_feature_names_out()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"4f6b5c2e-b5ea-41de-9943-da13c9b54e02"},{"cell_type":"code","source":"# These are the model properties.  You can call all of these\ndef get_properties(model):   \n  return [i for i in model.__dict__ if i.endswith('_')] \nget_properties(dt)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b77b032f-0766-48e7-ac77-1ba688e3ffc2"},{"cell_type":"code","source":"# Rough view of the tree - hard to read but can be usable for research.  Will show a clean tree later\nfrom sklearn.tree import export_text \n \ntree_text = export_text(dt, feature_names=dv.feature_names_) \nprint(tree_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"ad10a5b7-e27c-4df3-8fce-118889329a33"},{"cell_type":"code","source":"# Very important.  Once you have a trained model - interegate the coefficients to see WHAT is important\nfeature_names=dv.feature_names_\n# Evaluate the coefficients to learn what the model thinks is important in the predictions.\nfor i,j in zip(feature_names, dt.feature_importances_): print('%.3f' % j, i)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"a70eb705-485e-4e11-84f6-357949838a1a"},{"cell_type":"markdown","source":"# Confusion Matrix represent predictions vs Actuals on Test Data\nThe predicted data results in the below diagram could be read in the following manner given 1 represents  (positive) - obviously - this will be different if the target has more than 2 options. <br />\n![image info](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse1.mm.bing.net%2Fth%3Fid%3DOIP.g4tLqo_z92Hk7NZ-JXdziwHaD1%26pid%3DApi&f=1)\n\n### True Positive (TP): True positive represents the value of correct predictions of positives out of actual positive cases. \n### False Positive (FP): False positive represents the value of incorrect positive predictions. \n### True Negative (TN): True negative represents the value of correct predictions of negatives out of actual negative cases. \n## False Negative (FN): False negative represents the value of incorrect negative predictions.","metadata":{},"id":"aa665706-adc1-4873-8596-dc1302f4f280"},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nimport matplotlib.pyplot as plt\ny_pred = dt.predict_proba(X_test)[:, 1]\nconf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n#\n# Print the confusion matrix using Matplotlib\n#\nfig, ax = plt.subplots(figsize=(5, 5))\nax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\nfor i in range(conf_matrix.shape[0]):\n    for j in range(conf_matrix.shape[1]):\n        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n \nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix', fontsize=18)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"88b70daf-f5bf-4ce9-b6e1-71f8d4dc4c5e"},{"cell_type":"markdown","source":"## Precision: \nModel precision score represents the model’s ability to correctly predict the positives out of all the positive predictions it made. The precision score is a useful measure of the success of prediction when the classes are very imbalanced. Mathematically, it represents the ratio of true positive to the sum of true positive and false positive.\n#### Precision Score = TP / (FP + TP)","metadata":{},"id":"5205fe28-2550-46a5-b569-91eccbe10a36"},{"cell_type":"markdown","source":"The precision score can be used in the scenario where the machine learning model is required to identify all positive examples without any false positives. For example, machine learning models are used in medical diagnosis applications where the doctor wants machine learning model will not provide a label of pneumonia if the patient does not have this disease. Oncologists want models that can identify all cancerous lesions without any false-positive results, and hence one would use a precision score in such cases.\nThe other example where the precision score can be useful is credit card fraud detection. In credit card fraud detection problems, classification models are evaluated using the precision score to determine how many positive samples were correctly classified by the classification model. You would not like to have a high number of false positives or else you might end up blocking many credit cards and hence a lot of frustrations with the end-users.","metadata":{},"id":"be7c83ba-d39b-4070-a4de-341f920bf9a3"},{"cell_type":"code","source":"# Read this as a percentage - What proportion of positive identifications was actually correct?\nprint('Precision: %.3f' % precision_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b2f4d456-204f-4722-941b-85fdc7e94722"},{"cell_type":"markdown","source":"## Recall: \nModel recall score represents the model’s ability to correctly predict the positives out of actual positives. This is unlike precision which measures how many predictions made by models are actually positive out of all positive predictions made. For example: If your machine learning model is trying to identify positive reviews, the recall score would be what percent of those positive reviews did your machine learning model correctly predict as a positive. In other words, it measures how good our machine learning model is at identifying all actual positives out of all positives that exist within a dataset. The higher the recall score, the better the machine learning model is at identifying both positive and negative examples. Recall score is a useful measure of success of prediction when the classes are very imbalanced.  Mathematically, it represents the ratio of true positive to the sum of true positive and false negative.\n#### Recall Score = TP / (FN + TP)","metadata":{},"id":"6bed6652-5f63-4513-9497-2165b113596a"},{"cell_type":"markdown","source":"Recall score can be used in the scenario where the labels are not equally divided among classes. For example, if there is a class imbalance ratio of 20:80 (imbalanced data), then the recall score will be more useful than accuracy because it can provide information about how well the machine learning model identified rarer events.","metadata":{},"id":"7999d3e1-ff4d-4eba-904b-de982d867ec0"},{"cell_type":"code","source":"print('Recall: %.3f' % recall_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b9206c8c-f2c7-4a04-90a2-a2ff337c2021"},{"cell_type":"markdown","source":"## Model accuracy:\nis a machine learning model performance metric that is defined as the ratio of true positives and true negatives to all positive and negative observations. In other words, accuracy tells us how often we can expect our machine learning model will correctly predict an outcome out of the total number of times it made predictions. For example: Let’s assume that you were testing your machine learning model with a dataset of 100 records and that your machine learning model predicted all 90 of those instances correctly. The accuracy metric, in this case, would be: (90/100) = 90%. The accuracy rate is great but it doesn’t tell us anything about the errors our machine learning models make on new data we haven’t seen before.\nMathematically, it represents the ratio of the sum of true positive and true negatives out of all the predictions.  Use this measure with extreme caution.  It can be very misleading.\n#### Accuracy Score = (TP + TN)/ (TP + FN + TN + FP)","metadata":{},"id":"7b092ecd-6da8-420a-9070-6054e25e26f8"},{"cell_type":"code","source":"print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"85efd4d5-c615-4255-944a-855171d4a5ae"},{"cell_type":"markdown","source":"## Model F1 score:\nrepresents the model score as a function of precision and recall score. F-score is a machine learning model performance metric that gives equal weight to both the Precision and Recall for measuring its performance in terms of accuracy, making it an alternative to Accuracy metrics (it doesn’t require us to know the total number of observations). It’s often used as a single value that provides high-level information about the model’s output quality. This is a useful measure of the model in the scenarios where one tries to optimize either of precision or recall score and as a result, the model performance suffers. The following represents the aspects relating to issues with optimizing either precision or recall score:\nOptimizing for recall helps with minimizing the chance of not detecting a malignant cancer. However, this comes at the cost of predicting malignant cancer in patients although the patients are healthy (a high number of FP).\nOptimize for precision helps with correctness if the patient has a malignant cancer. However, this comes at the cost of missing malignant cancer more frequently (a high number of FN).\nMathematically, it can be represented as harmonic mean of precision and recall score.  Ranges from 0 to 1 and can be read as a percentage when multiplied by 100\n#### F1 Score = 2* Precision Score * Recall Score/ (Precision Score + Recall Score/)","metadata":{},"id":"e77df509-d411-4839-b0d3-e6c51b9ead00"},{"cell_type":"code","source":"# This is just for the TRUE --- Not the False\nprint('F1 Score: %.3f' % f1_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"c14e29a9-fc22-4789-9104-d1b1126fc4e7"},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"9d5b0aaa-10f7-4bb6-8047-4c944bf563da"},{"cell_type":"code","source":"# Look to see if your target variables are balanced.  If they are NOT then consider looking at Recall and Precision\n# If your target variables are balanced - then consider using accuracy as the defining metric to judge the \"goodness\"\ndf['revenue'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"98307b96-f042-4c02-8918-aa98f366f0a4"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"68ca66d4-70d6-4c9c-890e-7f2b4a751d1d"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"bf3920be-4979-4bde-948a-976c88a534bf"},{"cell_type":"code","source":"# Same info can be found in the score report\nfrom sklearn.metrics import f1_score\ny_pred = dt.predict_proba(X_test)[:, 1]\ny_pred = y_pred.astype('float')\nf1_score(y_test, y_pred, average=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"82acda9d-b8a1-4517-8592-d7def874041f"},{"cell_type":"code","source":"# Evalation Metrics (really more advanced but added here for context)\n# https://github.com/sepandhaghighi/pycm\n# Make sure the target variables in the y_test and y_pred are exactly the same units (not 0/1 in one and True/False in the other)\n!pip install pycm\nfrom pycm import ConfusionMatrix\ncm = ConfusionMatrix(actual_vector=y_test,predict_vector=y_pred)\n# cm = ConfusionMatrix(y_actu, y_pred, classes=[1,0,4])\nprint(cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"6fe5b7a3-0dd0-4848-94d4-75e42ce45518"},{"cell_type":"code","source":"# Classification metrics - you can skip or run and print the metric you woult like to look at.\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\ncnf_matrix = confusion_matrix(y_test, y_pred)\nprint(cnf_matrix)\n#[[1 1 3]\n# [3 2 2]\n# [1 3 1]]\n\nFP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \nFN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\nTP = np.diag(cnf_matrix)\nTN = cnf_matrix.sum() - (FP + FN + TP)\n\nFP = FP.astype(float)\n# print(FP)\nFN = FN.astype(float)\nTP = TP.astype(float)\nTN = TN.astype(float)\n\n# Sensitivity, hit rate, recall, or true positive rate\nTPR = TP/((TP+FN)+.01)\n# Specificity or true negative rate\nTNR = TN/((TN+FP)+.01)\n# Precision or positive predictive value\nPPV = TP/((TP+FP)+.01)\n# Negative predictive value\nNPV = TN/((TN+FN)+.01)\n# Fall out or false positive rate\nFPR = FP/((FP+TN)+.01)\n# False negative rate\nFNR = FN/((TP+FN)+.01)\n# False discovery rate\nFDR = FP/((TP+FP)+.01)\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b409e76c-e2b0-46b4-ba44-d6bbd475f2e6"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"78b66286-d96c-45c9-94f5-6edcbb7e4cd9"},{"cell_type":"markdown","source":"## Take a peek at your predictions","metadata":{},"id":"0e6940be-85fa-46e8-ab02-812409a8c13d"},{"cell_type":"code","source":"# Take a look at the first 10 rows and compare the predictions.\npred_y = dt.predict(X_test)\nprint(\"The first 10 prediction {}\".format(pred_y[:10].round(0)))\nprint(\"The real first 10 labels {}\".format(y_test[:10]))","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"4a0ca23f-81cd-4f0a-b815-66ab6d56dafd"},{"cell_type":"markdown","source":"## How to make a PREDICTION with brand new values","metadata":{},"id":"960544ef-d283-416f-b834-59b369c31ee3"},{"cell_type":"code","source":"type(df_train_full.head(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"6458b9b4-53f5-4325-8cc8-e316fb20eae0"},{"cell_type":"code","source":"# Use double brackets around the iloc to force it to return a pandas dataframe and not a series\n# Then you can convert ANY record into a dictionary.\n# This is important because when you want to make a prediction - it must be in this format\ndf_train_full.iloc[[21]]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"6cfd0fbc-b397-4c86-9be0-4d5b105c7a04"},{"cell_type":"code","source":"# How to convert any pandas row into a dictionary... needed for predictions\ndf_train_full.iloc[[213]].to_dict('records')[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"0b519a52-11f8-4d03-9cb2-fee16a79acbf"},{"cell_type":"code","source":"# How to convert any pandas row into a dictionary... needed for predictions\ndf_train_full.head(21).to_dict('records')[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"78bc6352-6bab-4356-9e6e-07c1a977a06f"},{"cell_type":"code","source":"#item = df_train.head(1).to_dict('records')[0]\nitem = df_train_full.iloc[[213]].to_dict('records')[0]\nactual = y_train[[213]]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8f58aa87-c643-4a94-b0ab-c0582ee4e18d"},{"cell_type":"code","source":"# The item to be predicted is passed in.  \ndef model_prediction(item, dv, model):\n    X = dv.transform([item])\n    y_pred = model.predict(X)\n    return y_pred[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"0b8da488-76d2-4c2e-88b4-59f0390517e4"},{"cell_type":"code","source":"model_prediction(item,dv,dt)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"25ee6832-7b4b-40f1-9f02-46f611d87968"},{"cell_type":"code","source":"actual","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"a2dbd283-4485-44e5-aa81-fdc0c0ebb813"},{"cell_type":"code","source":"# How to convert any pandas row into a dictionary... needed for predictions\ndf_train_full.iloc[[213]].to_dict('records')[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b6f61c86-9a23-4834-bcbd-af531740d9d2"},{"cell_type":"code","source":"# The list above can be copied into here.  Take everything after a \" : \" and put it in a [] so you can pass it in as a pd.DF\n# create a DataFrame by transforming scalar values to list   (toggle the weekend)\n\nmyItem = {'administrative': [6],\n 'administrative_duration': [94.6],\n 'informational': [0],\n 'informational_duration': [2.0],\n 'productrelated': [15],\n 'productrelated_duration': [1933.559259],\n 'bouncerates': [0.005333333],\n 'exitrates': [0.026377261],\n 'pagevalues': [167.806338478],\n 'specialday': [0.0],\n 'month': ['nov'],\n 'operatingsystems': [2],\n 'browser': [2],\n 'region': [4],\n 'traffictype': [2],\n 'visitortype': ['returning_visitor'],\n 'weekend': [False]}","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"a84c2cc4-a71a-451c-88f1-17e1beb840ab"},{"cell_type":"code","source":"newDF = pd.DataFrame.from_dict(myItem)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"2aea585a-21a3-47ec-8e8c-35667f2e6674"},{"cell_type":"code","source":"item = newDF.to_dict('records')[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"79a610e8-7d34-4902-9716-3033b5715bec"},{"cell_type":"code","source":"model_prediction(item,dv,dt)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8d65b86e-0bbe-47f5-bff1-04fc2cc0a1e2"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"ec91ee53-d4bd-455d-ac62-4bb7df7872a0"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"6c0fc8a6-309d-4458-b463-35c7129daa8f"},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{},"id":"a3376d80-ebf1-4a3b-9570-f01c2bb16cc8"},{"cell_type":"code","source":"# Look at the model parameters.   You can tune these.\ndt.get_params()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"940c8c4e-d164-4da0-8287-630f7306b78a"},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nparams = {'min_samples_leaf': [2, 3, 4,6,8,10,20,30,40,50],'max_depth': [2, 3, 4,6,8,10,20,30,40,50],'min_samples_split': [2,4,8]}\ngrid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3)\ngrid_search_cv.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"976b78b1-f559-48bb-b0fc-2ace1b82088f"},{"cell_type":"code","source":"grid_search_cv.best_estimator_    # this will output the best values for the hyperparameters","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"633aa3d9-1e39-4c1c-b889-1684f99e9825"},{"cell_type":"code","source":"# Let's assume that the decision tree is the one we want to explore\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=20, random_state=42)\ndt.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"104a253f-91ae-4005-92dd-3015a755b3b7"},{"cell_type":"code","source":"from sklearn.metrics import f1_score\ny_pred = dt.predict_proba(X_test)[:, 1]\ny_pred = y_pred.astype('int')\nf1_score(y_test, y_pred, average=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"9f985c0e-ae32-47e5-925b-ac2b237753be"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"c1ff5712-e49b-4eeb-8e90-47bca458db63"},{"cell_type":"code","source":"from sklearn.tree import export_graphviz\nexport_graphviz( \n grid_search_cv.best_estimator_,\n out_file=('tree.dot'),\n feature_names=dv.get_feature_names_out(),\n class_names=None,\n filled=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"bd4ea648-fadd-4604-92e0-c5d39657eb9c"},{"cell_type":"code","source":"!pip install pydot\nimport pydot\n\n(graph,) = pydot.graph_from_dot_file('tree.dot')\ngraph.write_png('tree.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"4b1d5356-f985-4b12-8982-b8107596c1b7"},{"cell_type":"code","source":"# You can change the params by editing the output of this and repeating the above steps.\ndt.get_params()","metadata":{},"execution_count":null,"outputs":[],"id":"44b575ad-7e8b-4486-98f3-4c70510aa742"},{"cell_type":"code","source":"#Many parameters will take a very long time to load\nparam = { 'max_depth': [2,3,5,20,40], \n         'max_leaf_nodes': [2,20,200]}","metadata":{},"execution_count":null,"outputs":[],"id":"f7881392-37cd-405b-8f80-3282f763a90e"},{"cell_type":"code","source":"metrics.SCORERS.keys()","metadata":{},"execution_count":null,"outputs":[],"id":"c46cb5ba-a070-47f1-9e17-7e61b44d1054"},{"cell_type":"code","source":"from sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define search\nsearch = GridSearchCV(dt, param, scoring='accuracy', n_jobs=-1, cv=cv)\n# execute search\nresult = search.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[],"id":"91bd0b8f-8fd4-4e9a-9823-d6f531790d63"},{"cell_type":"code","source":"# summarize result\nprint('Best Score: %s' % result.best_score_)\nprint('Best Hyperparameters: %s' % result.best_params_)","metadata":{},"execution_count":null,"outputs":[],"id":"46caaf03-88e3-4434-9ac2-0ce804f8e020"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"303e98c0-40ed-42c3-9c74-df96f0747223"}]}